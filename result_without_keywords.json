[
  {
    "id": "Journal.pdf_99a43459-166b-4b94-b363-0e018659fd98_chunk8",
    "url": "./unrelated/Journal.pdf",
    "text": "1310 HERMAN, A. SUSANTO, T. W. CENGGORO, D. ARIANSYAH AND B. PARDAMEAN\n[6] N. Jamil, A. Mohamed and S. Abdullah, Automated grading of palm oil fresh fruit bunches (FFB)\nusing neuro-fuzzy technique, 2009 International Conference of Soft Computing and Pattern Recog-\nnition, pp.245-249, 2009.\n[7] W. Ishak, R. Hudzari et al., Image based modeling for oil palm fruit maturity prediction, Journal\nof Food, Agriculture & Environment , vol.8, no.2, pp.469-476, 2010.\n[8] N. Fadilah and J. Mohamad-Saleh, Color feature extraction of oil palm fresh fruit bunch image for\nripeness classiﬁcation, The 13th Int. Conf. Appl. Comput. Appl. Comput. Sci. , pp.51-55, 2014.\n[9] Z. Ibrahim, N. Sabri and D. Isa, Palm oil fresh fruit bunch ripeness grading recognition using\nconvolutional neural network, Journal of Telecommunication, Electronic and Computer Engineering\n(JTEC), vol.10, no.3-2, pp.109-113, 2018.\n[10] Harsawardana, R. Rahutomo, B. Mahesworo, T. W. Cenggoro, A. Budiarto, T. Suparyanto, D. B. S.\nAtmaja, B. Samoedro and B. Pardamean, AI-based ripeness grading for oil palm fresh fruit bunch\nin smart crane grabber, IOP Conference Series: Earth and Environmental Science , vol.426, 012147,\n2020.\n[11] V. Mnih, N. Heess, A. Graves et al., Recurrent models of visual attention, Advances in Neural\nInformation Processing Systems , pp.2204-2212, 2014.\n[12] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel and Y. Bengio, Show,\nattend and tell: Neural image caption generation with visual attention, International Conference on\nMachine Learning, pp.2048-2057, 2015.\n[13] L.-C. Chen, Y. Yang, J. Wang, W. Xu and A. L. Yuille, Attention to scale: Scale-aware semantic\nimage segmentation, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition ,\npp.3640-3649, 2016.\n[14] Z. Chen, L. Liu, I. Sa, Z. Ge and M. Chli, Learning context ﬂexible attention model for long-term\nvisual place recognition, IEEE Robotics and Automation Letters , vol.3, no.4, pp.4015-4022, 2018.\n[15] Q. Tao, Z. Ge, J. Cai, J. Yin and S. See, Improving deep lesion detection using 3D contextual and\nspatial attention, International Conference on Medical Image Computing and Computer-Assisted\nIntervention, pp.185-193, 2019.\n[16] S. Woo, J. Park, J.-Y. Lee and I. S. Kweon, CBAM: Convolutional block attention module, Proc. of\nthe European Conference on Computer Vision (ECCV) , pp.3-19, 2018.\n[17] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho and Y. Bengio, Attention-based models for\nspeech recognition, Advances in Neural Information Processing Systems , pp.577-585, 2015.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser and I. Polosukhin,\nAttention is all you need, Advances in Neural Information Processing Systems , pp.5998-6008, 2017.\n[19] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, BERT: Pre-training of deep bidirectional trans-\nformers for language understanding, Proc. of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers) , pp.4171-4186, 2019.\n[20] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov and Q. V. Le, XLNet: Generalized\nautoregressive pretraining for language understanding, Advances in Neural Information Processing\nSystems, pp.5753-5763, 2019.\n[21] G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger, Densely connected convolutional\nnetworks, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition , pp.4700-\n4708, 2017.\n[22] K. He, X. Zhang, S. Ren and J. Sun, Deep residual learning for image recognition, Proc. of the IEEE\nConference on Computer Vision and Pattern Recognition , pp.770-778, 2016.\n[23] C. Yan, J. Yao, R. Li, Z. Xu and J. Huang, Weakly supervised deep learning for thoracic disease\nclassiﬁcation and localization on chest X-rays, Proc. of the 2018 ACM International Conference on\nBioinformatics, Computational Biology, and Health Informatics , pp.103-110, 2018.\n[24] B. Pardamean, Dasar Bioinformatika Dengan R (Fundamentals of Bioinformatics with R) , Graha\nIlmu, 2017 (in Indonesian).\n[25] B. Pardamean, A. Budiarto and R. Caraka, Bioinformatika Dengan R Tingkat Lanjut (Advanced\nBioinformatics with R) , Graha Ilmu, 2018 (in Indonesian).\nView publication stats",
    "page": 8,
    "score": 0.75
  },
  {
    "id": "Journal.pdf_c3ed0bc0-319c-4daa-b877-07911ea6483e_chunk0",
    "url": "./unrelated/Journal.pdf",
    "text": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/376166986\nDEEP LEARNING WITH AUXILIARY LEARNING ATTENTION MECHANISM FOR\nOIL PALM FRUIT IMAGE RIPENESS CLASSIFICATION\nArticle  in   ICIC Express Letters · December 2023\nDOI: 10.24507/icicel.17.12.1303\nCITATIONS\n2\nREADS\n205\n5 authors, including:\nHerman Herman\nBinus University\n10 PUBLICATIONS   77 CITATIONS   \nSEE PROFILE\nAlbert Susanto\nBinus University\n10 PUBLICATIONS   77 CITATIONS   \nSEE PROFILE\nTjeng Wawan Cenggoro\nBinus University\n141 PUBLICATIONS   1,923 CITATIONS   \nSEE PROFILE\nDedy Ariansyah\nBinus University\n36 PUBLICATIONS   386 CITATIONS   \nSEE PROFILE\nAll content following this page was uploaded by Bens Pardamean on 02 December 2023.\nThe user has requested enhancement of the downloaded file.",
    "page": 0,
    "score": 0.6478903480154052
  },
  {
    "id": "Journal.pdf_32767a01-f631-4073-8b8f-19dbb972444c_chunk7",
    "url": "./unrelated/Journal.pdf",
    "text": "ICIC EXPRESS LETTERS, VOL.17, NO.12, 2023 1309\nTable 5.DenseNet confusion matrix\nPrediction\nGround truth\nBP BM KM MKM M MM TM\nBP 36 0 0 0 0 0 0\nBM 0 12 0 0 0 0 0\nKM 0 0 86 20 0 31 7\nMKM 1 0 0 35 0 0 0\nM 2 0 0 0 216 8 2\nMM 7 0 8 0 18 358 5\nTM 1 0 0 0 10 11 50\nTable 6.AuxNet confusion matrix\nPrediction\nGround truth\nBP BM KM MKM M MM TM\nBP 24 0 0 0 0 12 0\nBM 0 12 0 0 0 0 0\nKM 0 0 127 12 0 5 0\nMKM 0 0 12 24 0 0 0\nM 0 0 15 0 192 9 12\nMM 0 0 11 0 18 367 0\nTM 0 0 0 0 13 0 59\n5. Conclusion. The attention generated from auxiliary learning could help the model\nlearn better by outputting what it is being processed in the middle of the network during\nbackpropagation. With the help of the attention block, the model could also produce a\nmore focused dot product in which each feature has its important region rather than the\noverall features. The extension is able to increase the base model (DenseNet) accuracy,\nprecision, recall, and F1 Score. The improvement over the DenseNet performance is 2%,\n1%, 1%, and 2% respectively on accuracy, precision, recall, and F1 score. In the future,\nit is suggested to integrate ALAM on self-attention mechanism in a Visual-Transformers-\nbased model.\nAcknowledgment. The experiments in this study were executed using NVIDIA Tesla\nP4 and P100 supported by NVIDIA – BINUS AI R&D Center.\nREFERENCES\n[1] T. W. Cenggoro, A. Budiarto, R. Rahutomo and B. Pardamean, Information system design for deep\nlearning based plant counting automation, 2018 Indonesian Association for Pattern Recognition\nInternational Conference (INAPR) , pp.329-332, 2018.\n[2] H. Sastrohartono, A. P. Suryotomo, S. Saifullah, T. Suparyanto, A. S. Perbangsa and B. Pardamean,\nDrone application model for image acquisition of plantation areas and oil palm trees counting,\n2022 International Conference on Information Management and Technology (ICIMTech) , pp.167-\n171, 2022.\n[3] R. A. Pratama and T. Widodo, The impact of nontariﬀ trade policy of European Union crude\npalm oil import on Indonesia, Malaysia, and the rest of the world economy: An analysis in GTAP\nframework, Jurnal Ekonomi Indonesia , vol.9, no.1, pp.39-52, 2020.\n[4] H. Herman, T. W. Cenggoro, A. Susanto and B. Pardamean, Deep learning for oil palm fruit ripeness\nclassiﬁcation with DenseNet, 2021 International Conference on Information Management and Tech-\nnology (ICIMTech), vol.1, pp.116-119, 2021.\n[5] M. S. M. Alfatni, A. R. M. Shariﬀ, H. Z. M. Shafri, O. B. Saaed, O. M. Eshanta et al., Oil palm\nfruit bunch grading system using red, green and blue digital number, Journal of Applied Sciences ,\nvol.8, no.8, pp.1444-1452, 2008.",
    "page": 7,
    "score": 0.5525585986125314
  },
  {
    "id": "Journal.pdf_437a6143-3eb4-4c84-9fc4-0d688f92d374_chunk3",
    "url": "./unrelated/Journal.pdf",
    "text": "ICIC EXPRESS LETTERS, VOL.17, NO.12, 2023 1305\n3.1. DenseNet. DenseNet [21] is a network that utilizes dense connections, whose idea\nis to concatenate the feature map of all previous layers to the subsequent layer in a\nblock. The dense connections allow DenseNet to reuse previous feature maps, which ease\nthe training process. DenseNet is also more parameter-eﬃcient than ResNet [22], which\nconnects only the output of the immediate previous layer. Figure 2 illustrates the inner\nworking of DenseNet.\nFigure 2. DenseNet architecture\n3.2. Auxiliary learning attention mechanism.Attention mechanism is usually im-\nplemented as a learned feature map that is multiplied element-wise to the ﬁnal feature\nmaps of a CNN. To model attention with the learned feature map, it usually goes through\na softmax or sigmoid activation function. Attaching an attention module only to the ﬁnal\nfeature maps limits the expressiveness of the model. However, attaching it to intermediate\nfeature maps leads to an architecture that requires gradient from earlier layers to pass\nthrough softmax/sigmoid function multiple times. Because the magnitude of the gradients\nis decreased at every pass, the model is prone to heavily suﬀer from vanishing gradient\nproblem.\nTo cope with this issue, we proposed an attention mechanism module that throws its\noutput to an auxiliary classiﬁer instead of passing it to the next layer. We call this atten-\ntion mechanism as Auxiliary Learning Attention Mechanism (ALAM). Figure 3 illustrates\nFigure 3. AuxNet",
    "page": 3,
    "score": 0.48045396957071196
  },
  {
    "id": "Journal.pdf_efb94b1b-d291-40e1-b5f8-b89b6bbe3fe6_chunk4",
    "url": "./unrelated/Journal.pdf",
    "text": "1306 HERMAN, A. SUSANTO, T. W. CENGGORO, D. ARIANSYAH AND B. PARDAMEAN\nour proposed model, where the attention modules are attached to the output of each block\nin a DenseNet architecture. ALAM uses a single Conv2D that outputs the same tensor\nsize as the input before going through the Sigmoid activation function which produces\nthe attention area. Each tensor will retain its own attention as each channel focuses on\neach of their features. The attention tensor will then be multiplied element-wise with the\noriginal input before going through a Global Average Pool. The dot product will then be\nconverted into a fully connected layer that will classify the image. The only block that did\nnot use attention is the last one as the network does not continue to another block after-\nward. The proposed network that uses ALAM is referred as AuxNet (Auxiliary Network)\nfor the rest of this paper.\nTo benchmark our proposed model, we compared it to a standard AlexNet that was\nused by Ibrahim et al. [9], a standard DenseNet, and a DenseNet with SE modules. All\nmodels used a pretrained ImageNet model to have beneﬁts from transfer learning. The\nlatter was used in the benchmarking experiment to test whether our proposed attention\nmodule is better than the state-of-the-art attention module. We employed the DenseNet\nwith SE modules introduced by Yan et al. [23], which enables the use of a pretrained\nDenseNet.\nIn the benchmarking experiment, we used a dataset of 400 images oil palm FFB images,\ncategorized into 7 ripeness classes. The distribution of images in the dataset is summarized\nin Table 1. The dataset was dividied into 3 groups for a typical CNN training procedure:\ntraining data (64%, 256 images), validation (20%, 80 images), and testing (16%, 64 im-\nages). To increase the variation in the dataset, all models were trained with Ten Crop, a\ndata augmentation procedure that can increase a dataset variation ten-fold. It works by\ncropping the top-left, top-right, bottom-left, bottom-right, and center part of an image\nto produce ﬁve diﬀerent images. These ﬁve images are also ﬂipped horizontally to obtain\nmore images.\nTable 1.Classiﬁcation of oil palm fruit\nName Description Total\nBP Ripening 16\nBM Raw 8\nKM Less Ripped 64\nMKM Almost Ripped 16\nM Ripped 96\nMM Perfectly Ripped 168\nTM Excessively Ripped 32\nTotal 400\n4. Results and Discussion.Table 2 shows the evaluation result of all models perfor-\nmance in the benchmarking experiment. It is clear that the models with DenseNet are\nsuperior from AlexNet. Interestingly, adding SE modules to the standard DenseNet re-\nduced the performance. This phenomenon can be explained by the fact that an SE module\npasses its output to the next layer, which has been exposed to a softmax function. This\ndesign allows the gradients of earlier layer to pass through several softmax function, re-\nducing the gradients magnitude for each pass. This leads to a vanishing gradient problem.\nContrarily to the SE module, adding ALAM module improved the performance of the\nstandard DenseNet. This was possible by the fact that ALAM escapes its output to a\nclassiﬁer instead of passing it to the next layer. This guarantees any gradients within the\nmodel to only pass an ALAM module once.\nTop-1 error rate and F1 Score are the evaluation methods used to measure the per-\nformance of all models proposed. All architecture was implemented with PyTorch under",
    "page": 4,
    "score": 0.4549970888851518
  },
  {
    "id": "Journal.pdf_274876eb-c8f6-46f9-b245-e38cd666c9df_chunk2",
    "url": "./unrelated/Journal.pdf",
    "text": "1304 HERMAN, A. SUSANTO, T. W. CENGGORO, D. ARIANSYAH AND B. PARDAMEAN\nno modiﬁcation [9, 10]. It is possible that modifying the architecture to be more suitable\nfor oil palm FFB classiﬁcation would result in a better performance.\nAmong the possible modiﬁcation for deep learning architectures, attention mechanism\nis the most promising to be applied. It is deﬁned as a module that lets the deep learning\nmodel strongly attend only part of the image to classify images, as illustrated in Figure 1.\nThis module is potentially suitable for oil palm FFB classiﬁcation, because the ripeness\nof the FFB can be determined only by looking at the color of the fruits, not the image in\ngeneral. Thus, the contribution of this study is to design an attention mechanism module\nthat can improve the accuracy of a standard deep-learning-based image classiﬁcation\nmodel for oil palm FFB classiﬁcation. The study is organized as follows. The background\nand related works are presented in Sections 1 and 2, followed by methodology in Section\n3. Section 4 presents the results and discussion, and Section 5 concludes the ﬁndings of\nthe study.\nFigure 1. Eggplant detection attention\n2. Related Works.Multiple researches have been conducted to develop deep learning\nmodels with attention mechanism. The ﬁrst attempt to apply the attention mechanism\ninto deep learning was done by Mnih et al. [11] for image classiﬁcation. The attention was\ngenerated via recurrent neural network, which is applied afterward to generate caption\nfrom an image [12]. For image captioning, two types of attention were developed: a\nsoft attention mechanism that generates continuous value as the attention and a hard\nattention that generates binary value as the attention. Inspired by this study, various\nattention mechanism modules were developed for computer vision cases such as image\nsegmentation [13], place recognition [14], medical image analysis [15], and also general\nimage classiﬁcation.\nSpeciﬁcally for general image classiﬁcation, the current most notable attention module is\nSqueeze and Excitation (SE) module. It injects a convolutional layer a capability to attend\nto its output feature maps channel-wise. In other words, it can be called as a channel\nattention. It successfully improved the accuracy of various standard Convolution Neural\nNetwork (CNN) for image classiﬁcation on ImageNet dataset. SE is extended by Woo et\nal. [16] with the addition of spatial attention module on top of the channel attention. The\nextended module is called Convolutional Block Attention Module (CBAM).\nNot only for computer vision, attention mechanism is also emerging for speech recog-\nnition [17]. It is even considered as a monumental breakthrough in natural language\nprocessing, with the formulation of self-attention [18], which gave birth to a new mod-\nel called Transformer. It is currently the base of almost all popular models in natural\nlanguage processing, such as Bidirectional Encoder Representations from Transformers\n(BERT) [19] and Extra-Long Neural Network (XL-Net) [20].\n3. Proposed Model. This section discusses the proposed model named AuxNet, which\nuses DenseNet as the backbone network.",
    "page": 2,
    "score": 0.4488253820878989
  },
  {
    "id": "Journal.pdf_32e472d9-7fea-4fc2-9422-893eafd545d9_chunk5",
    "url": "./unrelated/Journal.pdf",
    "text": "ICIC EXPRESS LETTERS, VOL.17, NO.12, 2023 1307\nTable 2.Test F1 Score\nModel name Accuracy Precision Recall F1 Score\nAlexNet 0.77 0.78 0.77 0.77\nDenseNet 0.85 0.87 0.86 0.85\nDenseNet + SE Layer 0.80 0.84 0.81 0.81\nAuxNet 0.87 0.88 0.87 0.87\nthe same python environment on NVIDIA Tesla P100 and Tesla P4 GPU provided by\nNVIDIA – BINUS AI R&D Center. All models weight values were taken based on Ima-\ngeNet transfer learning, so the model has already learned multiple features from general\nobjects. The hyperparameters conﬁg can be seen in Table 3.\nTable 3.Hyperparameter conﬁg\nHyperparameter Value\nLearning Rate (LR) 0.001\nLR decay steps Every 8 steps dropped for 10-1\nOptimization SGD\nBatch size 8\nEpochs 50\nPretrained True\nFreeze False\nThe LR used is the most common and basic value that is widely used in every deep\nlearning experiment. The learning steps have been tested both on every 8 and 30 epochs\nwith no signiﬁcant results changed; therefore, 8 epochs were used as the default for every\nmodel tested. The optimization used is Stochastic Gradient Descent (SGD), since it tends\nto work better on similar data between classes. The freeze was set to False so the network\ncould continue learning new features on top of pretrained from ImageNet.\nUsing one of the current state-of-the-art architectures for the image classiﬁcation task,\nDenseNet, with a deeper and complex model, it showed more promising results as seen in\nFigure 4. Therefore, all of the visual attention methods proposed were tested on DenseNet,\nFigure 4. DenseNet training graph",
    "page": 5,
    "score": 0.32255640696958254
  }
]